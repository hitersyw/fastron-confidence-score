{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 2)\n",
      "(800,)\n",
      "(200, 2)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset = \"CSpace2\"\n",
    "\n",
    "training_file = \"./data/%s_training.csv\" % dataset\n",
    "test_file = \"./data/%s_test.csv\" % dataset\n",
    "\n",
    "\n",
    "training_data = np.loadtxt(training_file, skiprows=3, delimiter=',')\n",
    "test_data = np.loadtxt(test_file, skiprows=3, delimiter=',')\n",
    "\n",
    "training_X = training_data[:, 0:2]\n",
    "training_y = training_data[:, 2]\n",
    "# training_y[training_y == -1] = 0 # convert y to 0;\n",
    "\n",
    "test_X = test_data[:, 0:2]\n",
    "test_y = test_data[:, 2]\n",
    "# test_y[test_y == -1] = 0 # convert y to 0;\n",
    "\n",
    "print(training_X.shape)\n",
    "print(training_y.shape)\n",
    "\n",
    "print(test_X.shape,)\n",
    "print(test_y.shape,)\n",
    "\n",
    "# print(\"Number of positive vs negative samples: %s vs %s\" % (np.count_nonzero(training_y == 1), np.count_nonzero(training_y == -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y_pred', -1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import json\n",
    "\n",
    "lr_output_file = \"./logRegression_%s.json\" % dataset\n",
    "\n",
    "lr_clf = LogisticRegression(random_state=0, solver='lbfgs',verbose=1).fit(training_X, training_y)\n",
    "y_lr = lr_clf.predict(test_X)\n",
    "p_lr = lr_clf.predict_proba(test_X).T # Need to align the first axis with y_pred\n",
    "\n",
    "header_lr = ['y_pred'] + lr_clf.classes_.tolist()\n",
    "print(header_lr)\n",
    "output_lr_array = np.vstack((y_lr, p_lr)).T\n",
    "output_lr = {\n",
    "    'coef': lr_clf.coef_.tolist(),\n",
    "    'intercept': lr_clf.intercept_.tolist(),\n",
    "    'test_output_header': header_lr,\n",
    "    'test_output': output_lr_array.tolist()\n",
    "}\n",
    "\n",
    "with open(lr_output_file, 'w') as outfile:\n",
    "    json.dump(output_lr, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 8.73, NNZs: 2, Bias: 61.115077, T: 800, Avg. loss: 0.555963\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 16.08, NNZs: 2, Bias: 58.040573, T: 1600, Avg. loss: 0.321125\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.96, NNZs: 2, Bias: 59.717566, T: 2400, Avg. loss: 0.243762\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 18.94, NNZs: 2, Bias: 55.956834, T: 3200, Avg. loss: 0.156149\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11.36, NNZs: 2, Bias: 58.217413, T: 4000, Avg. loss: 0.162834\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 12.67, NNZs: 2, Bias: 57.987640, T: 4800, Avg. loss: 0.092928\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 17.90, NNZs: 2, Bias: 55.545151, T: 5600, Avg. loss: 0.130886\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 15.29, NNZs: 2, Bias: 55.964399, T: 6400, Avg. loss: 0.123278\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 17.33, NNZs: 2, Bias: 54.979448, T: 7200, Avg. loss: 0.093483\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 14.77, NNZs: 2, Bias: 55.642334, T: 8000, Avg. loss: 0.057454\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 16.71, NNZs: 2, Bias: 54.687577, T: 8800, Avg. loss: 0.080960\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 13.51, NNZs: 2, Bias: 55.390975, T: 9600, Avg. loss: 0.071248\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 16.52, NNZs: 2, Bias: 54.138794, T: 10400, Avg. loss: 0.076733\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 16.13, NNZs: 2, Bias: 53.848435, T: 11200, Avg. loss: 0.079308\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 17.89, NNZs: 2, Bias: 52.943671, T: 12000, Avg. loss: 0.054809\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 15.75, NNZs: 2, Bias: 53.273050, T: 12800, Avg. loss: 0.073353\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 16.94, NNZs: 2, Bias: 52.587559, T: 13600, Avg. loss: 0.049940\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 15.60, NNZs: 2, Bias: 52.700708, T: 14400, Avg. loss: 0.052081\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 15.46, NNZs: 2, Bias: 52.564092, T: 15200, Avg. loss: 0.035863\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 15.11, NNZs: 2, Bias: 52.423766, T: 16000, Avg. loss: 0.053156\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 15.08, NNZs: 2, Bias: 52.135152, T: 16800, Avg. loss: 0.059503\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 15.18, NNZs: 2, Bias: 51.860198, T: 17600, Avg. loss: 0.041977\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 16.02, NNZs: 2, Bias: 51.377370, T: 18400, Avg. loss: 0.040367\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 15.37, NNZs: 2, Bias: 51.358160, T: 19200, Avg. loss: 0.043655\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 24 epochs took 0.02 seconds\n",
      "['y_pred', -1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import json\n",
    "\n",
    "sgd_output_file = \"./sgd_%s.json\" % dataset\n",
    "\n",
    "sgd_clf = SGDClassifier(loss='log', alpha=1e-4, random_state=0, verbose=1).fit(training_X, training_y)\n",
    "y_lr = sgd_clf.predict(test_X)\n",
    "p_lr = sgd_clf.predict_proba(test_X).T # Need to align the first axis with y_pred\n",
    "\n",
    "header_sgd = ['y_pred'] + sgd_clf.classes_.tolist()\n",
    "print(header_lr)\n",
    "output_sgd_array = np.vstack((y_lr, p_lr)).T\n",
    "output_sgd = {\n",
    "    'coef': sgd_clf.coef_.tolist(),\n",
    "    'intercept': sgd_clf.intercept_.tolist(),\n",
    "    'test_output_header': header_sgd,\n",
    "    'test_output': output_sgd_array.tolist()\n",
    "}\n",
    "\n",
    "with open(sgd_output_file, 'w') as outfile:\n",
    "    json.dump(output_sgd, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.48246091\n",
      "Iteration 2, loss = 0.21618065\n",
      "Iteration 3, loss = 0.11123294\n",
      "Iteration 4, loss = 0.07108934\n",
      "Iteration 5, loss = 0.05971234\n",
      "Iteration 6, loss = 0.05599833\n",
      "Iteration 7, loss = 0.05379363\n",
      "Iteration 8, loss = 0.05168343\n",
      "Iteration 9, loss = 0.04920830\n",
      "Iteration 10, loss = 0.04587292\n",
      "Iteration 11, loss = 0.04389534\n",
      "Iteration 12, loss = 0.04180975\n",
      "Iteration 13, loss = 0.03877963\n",
      "Iteration 14, loss = 0.03719557\n",
      "Iteration 15, loss = 0.03542967\n",
      "Iteration 16, loss = 0.03414566\n",
      "Iteration 17, loss = 0.03297715\n",
      "Iteration 18, loss = 0.03158088\n",
      "Iteration 19, loss = 0.03038861\n",
      "Iteration 20, loss = 0.02946444\n",
      "Iteration 21, loss = 0.02814407\n",
      "Iteration 22, loss = 0.02705257\n",
      "Iteration 23, loss = 0.02602492\n",
      "Iteration 24, loss = 0.02502616\n",
      "Iteration 25, loss = 0.02420860\n",
      "Iteration 26, loss = 0.02325380\n",
      "Iteration 27, loss = 0.02245297\n",
      "Iteration 28, loss = 0.02180310\n",
      "Iteration 29, loss = 0.02125769\n",
      "Iteration 30, loss = 0.02030619\n",
      "Iteration 31, loss = 0.01968929\n",
      "Iteration 32, loss = 0.01938467\n",
      "Iteration 33, loss = 0.01844872\n",
      "Iteration 34, loss = 0.01797242\n",
      "Iteration 35, loss = 0.01756714\n",
      "Iteration 36, loss = 0.01704902\n",
      "Iteration 37, loss = 0.01674731\n",
      "Iteration 38, loss = 0.01618773\n",
      "Iteration 39, loss = 0.01584546\n",
      "Iteration 40, loss = 0.01544927\n",
      "Iteration 41, loss = 0.01509503\n",
      "Iteration 42, loss = 0.01483888\n",
      "Iteration 43, loss = 0.01440409\n",
      "Iteration 44, loss = 0.01421075\n",
      "Iteration 45, loss = 0.01381308\n",
      "Iteration 46, loss = 0.01351385\n",
      "Iteration 47, loss = 0.01323353\n",
      "Iteration 48, loss = 0.01304596\n",
      "Iteration 49, loss = 0.01292090\n",
      "Iteration 50, loss = 0.01256679\n",
      "Iteration 51, loss = 0.01237211\n",
      "Iteration 52, loss = 0.01210295\n",
      "Iteration 53, loss = 0.01188608\n",
      "Iteration 54, loss = 0.01176265\n",
      "Iteration 55, loss = 0.01141239\n",
      "Iteration 56, loss = 0.01181051\n",
      "Iteration 57, loss = 0.01132932\n",
      "Iteration 58, loss = 0.01127686\n",
      "Iteration 59, loss = 0.01078220\n",
      "Iteration 60, loss = 0.01078985\n",
      "Iteration 61, loss = 0.01070828\n",
      "Iteration 62, loss = 0.01034514\n",
      "Iteration 63, loss = 0.01035220\n",
      "Iteration 64, loss = 0.01008728\n",
      "Iteration 65, loss = 0.01007058\n",
      "Iteration 66, loss = 0.00987426\n",
      "Iteration 67, loss = 0.00975641\n",
      "Iteration 68, loss = 0.00971298\n",
      "Iteration 69, loss = 0.00951114\n",
      "Iteration 70, loss = 0.00949921\n",
      "Iteration 71, loss = 0.00929552\n",
      "Iteration 72, loss = 0.00957503\n",
      "Iteration 73, loss = 0.00904517\n",
      "Iteration 74, loss = 0.00954029\n",
      "Iteration 75, loss = 0.00916703\n",
      "Iteration 76, loss = 0.00913732\n",
      "Iteration 77, loss = 0.00900327\n",
      "Iteration 78, loss = 0.00908012\n",
      "Iteration 79, loss = 0.00861255\n",
      "Iteration 80, loss = 0.00843385\n",
      "Iteration 81, loss = 0.00871194\n",
      "Iteration 82, loss = 0.00844642\n",
      "Iteration 83, loss = 0.00861345\n",
      "Iteration 84, loss = 0.00824750\n",
      "Iteration 85, loss = 0.00997474\n",
      "Iteration 86, loss = 0.00905951\n",
      "Iteration 87, loss = 0.00903692\n",
      "Iteration 88, loss = 0.00771393\n",
      "Iteration 89, loss = 0.00772940\n",
      "Iteration 90, loss = 0.00820735\n",
      "Iteration 91, loss = 0.00808627\n",
      "Iteration 92, loss = 0.00799669\n",
      "Iteration 93, loss = 0.00740173\n",
      "Iteration 94, loss = 0.00742154\n",
      "Iteration 95, loss = 0.00727019\n",
      "Iteration 96, loss = 0.00725915\n",
      "Iteration 97, loss = 0.00715613\n",
      "Iteration 98, loss = 0.00710159\n",
      "Iteration 99, loss = 0.00700662\n",
      "Iteration 100, loss = 0.00726614\n",
      "Iteration 101, loss = 0.00722289\n",
      "Iteration 102, loss = 0.00693843\n",
      "Iteration 103, loss = 0.00688272\n",
      "Iteration 104, loss = 0.00674477\n",
      "Iteration 105, loss = 0.00696880\n",
      "Iteration 106, loss = 0.00660645\n",
      "Iteration 107, loss = 0.00651398\n",
      "Iteration 108, loss = 0.00692978\n",
      "Iteration 109, loss = 0.00629381\n",
      "Iteration 110, loss = 0.00649914\n",
      "Iteration 111, loss = 0.00644827\n",
      "Iteration 112, loss = 0.00645313\n",
      "Iteration 113, loss = 0.00619018\n",
      "Iteration 114, loss = 0.00622096\n",
      "Iteration 115, loss = 0.00608029\n",
      "Iteration 116, loss = 0.00595522\n",
      "Iteration 117, loss = 0.00626426\n",
      "Iteration 118, loss = 0.00620767\n",
      "Iteration 119, loss = 0.00605653\n",
      "Iteration 120, loss = 0.00594991\n",
      "Iteration 121, loss = 0.00573862\n",
      "Iteration 122, loss = 0.00580545\n",
      "Iteration 123, loss = 0.00561890\n",
      "Iteration 124, loss = 0.00562006\n",
      "Iteration 125, loss = 0.00586474\n",
      "Iteration 126, loss = 0.00558990\n",
      "Iteration 127, loss = 0.00594172\n",
      "Iteration 128, loss = 0.00549882\n",
      "Iteration 129, loss = 0.00526134\n",
      "Iteration 130, loss = 0.00541718\n",
      "Iteration 131, loss = 0.00530772\n",
      "Iteration 132, loss = 0.00534380\n",
      "Iteration 133, loss = 0.00528310\n",
      "Iteration 134, loss = 0.00590376\n",
      "Iteration 135, loss = 0.00532917\n",
      "Iteration 136, loss = 0.00538998\n",
      "Iteration 137, loss = 0.00493022\n",
      "Iteration 138, loss = 0.00494661\n",
      "Iteration 139, loss = 0.00505893\n",
      "Iteration 140, loss = 0.00494069\n",
      "Iteration 141, loss = 0.00505995\n",
      "Iteration 142, loss = 0.00489198\n",
      "Iteration 143, loss = 0.00506722\n",
      "Iteration 144, loss = 0.00487506\n",
      "Iteration 145, loss = 0.00493463\n",
      "Iteration 146, loss = 0.00457570\n",
      "Iteration 147, loss = 0.00472856\n",
      "Iteration 148, loss = 0.00455656\n",
      "Iteration 149, loss = 0.00446958\n",
      "Iteration 150, loss = 0.00506523\n",
      "Iteration 151, loss = 0.00445220\n",
      "Iteration 152, loss = 0.00499530\n",
      "Iteration 153, loss = 0.00450389\n",
      "Iteration 154, loss = 0.00458868\n",
      "Iteration 155, loss = 0.00436548\n",
      "Iteration 156, loss = 0.00441382\n",
      "Iteration 157, loss = 0.00459620\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "['y_pred', -1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_output_file = \"./mlp_%s.json\" % dataset\n",
    "\n",
    "# TODO: Do we need to transform the output from {-1, 1} to {0, 1}?\n",
    "mlp_clf = MLPClassifier(solver='adam', alpha=1e-4, activation='relu',hidden_layer_sizes=(256, 256),\n",
    "                        random_state=1, max_iter=1000, verbose=True).fit(training_X, training_y)\n",
    "y_mlp = mlp_clf.predict(test_X)\n",
    "p_mlp = mlp_clf.predict_proba(test_X).T\n",
    "\n",
    "header_mlp = ['y_pred'] + mlp_clf.classes_.tolist()\n",
    "print(header_mlp)\n",
    "output_mlp_array = np.vstack((y_mlp, p_mlp)).T\n",
    "\n",
    "coefs = []\n",
    "intercepts = []\n",
    "for coef in mlp_clf.coefs_:\n",
    "    coefs.append(coef.tolist())\n",
    "\n",
    "for intercept in mlp_clf.intercepts_:\n",
    "    intercepts.append(intercept.tolist())\n",
    "\n",
    "output_mlp = {\n",
    "    'coef': coefs,\n",
    "    'intercept': intercepts,\n",
    "    'test_output_header': header_mlp,\n",
    "    'test_output': output_mlp_array.tolist()\n",
    "}\n",
    "\n",
    "with open(mlp_output_file, 'w') as outfile:\n",
    "    json.dump(output_mlp, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 256)\n",
      "(256, 256)\n",
      "(256, 1)\n",
      "(256,)\n",
      "(256,)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "for coef in mlp_clf.coefs_:\n",
    "    print(coef.shape,)\n",
    "    \n",
    "for b in mlp_clf.intercepts_:\n",
    "    print(b.shape,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ConfidenceScore] *",
   "language": "python",
   "name": "conda-env-ConfidenceScore-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
