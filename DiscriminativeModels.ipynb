{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 2)\n",
      "(600,)\n",
      "(200, 2)\n",
      "(200,)\n",
      "(200, 2)\n",
      "(200,)\n",
      "(array([0., 1.]), array([124, 476]))\n",
      "(array([0., 1.]), array([ 49, 151]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset = \"CSpace1\"\n",
    "\n",
    "training_file = \"./data/%s_training.csv\" % dataset\n",
    "holdout_file = \"./data/%s_validation.csv\" % dataset\n",
    "test_file = \"./data/%s_test.csv\" % dataset\n",
    "\n",
    "\n",
    "training_data = np.loadtxt(training_file, skiprows=0, delimiter=',')\n",
    "validation_data = np.loadtxt(holdout_file, skiprows=0, delimiter=',')\n",
    "test_data = np.loadtxt(test_file, skiprows=0, delimiter=',')\n",
    "\n",
    "training_X = training_data[:, 0:2]\n",
    "training_y = training_data[:, 2]\n",
    "training_y[training_y == -1] = 0 # convert y to 0;\n",
    "\n",
    "validation_X = validation_data[:, 0:2]\n",
    "validation_y = validation_data[:, 2]\n",
    "validation_y[validation_y == -1] = 0\n",
    "\n",
    "test_X = test_data[:, 0:2]\n",
    "test_y = test_data[:, 2]\n",
    "test_y[test_y == -1] = 0 # convert y to 0;\n",
    "\n",
    "print(training_X.shape)\n",
    "print(training_y.shape)\n",
    "\n",
    "print(validation_X.shape)\n",
    "print(validation_y.shape)\n",
    "\n",
    "print(test_X.shape,)\n",
    "print(test_y.shape,)\n",
    "\n",
    "print(np.unique(training_y, return_counts=True))\n",
    "print(np.unique(test_y, return_counts=True))\n",
    "# print(\"Number of positive vs negative samples: %s vs %s\" % (np.count_nonzero(training_y == 1), np.count_nonzero(training_y == -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for rbf passed.\n",
      "Shape of K for training, test, validation: ((600, 600), (200, 600), (200, 600))\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def rbf(x1, x2, g=1):\n",
    "    return np.exp(-g*(cdist(x1, x2, 'euclidean')**2))\n",
    "\n",
    "def testrbf():\n",
    "    x1 = np.array([[0, 1], [0, 2]])\n",
    "    x2 = np.array([[1, 0], [2, 0]])\n",
    "    out = rbf(x1,x2, 1)\n",
    "    expected = np.array([[np.exp(-2), np.exp(-5)], [np.exp(-5), np.exp(-8)]])\n",
    "    assert np.allclose(out, expected), \"RBF output are not equal!. Expected: %s; Actual: %s\" % (expected, out)\n",
    "    print(\"Test for rbf passed.\")\n",
    "    \n",
    "testrbf()\n",
    "\n",
    "g = 1;\n",
    "K_train = rbf(training_X, training_X, g)\n",
    "K_validation = rbf(validation_X, training_X)\n",
    "K_test = rbf(test_X, training_X)\n",
    "\n",
    "print(\"Shape of K for training, test, validation: (%s, %s, %s)\" % (K_train.shape, K_validation.shape, K_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 166.28, NNZs: 600, Bias: 56.882541, T: 600, Avg. loss: 11.468485\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 155.80, NNZs: 600, Bias: 47.830188, T: 1200, Avg. loss: 4.406536\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 147.24, NNZs: 600, Bias: 43.322377, T: 1800, Avg. loss: 2.927773\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 130.30, NNZs: 600, Bias: 48.185511, T: 2400, Avg. loss: 2.119621\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 122.43, NNZs: 600, Bias: 45.754005, T: 3000, Avg. loss: 1.837614\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 116.82, NNZs: 600, Bias: 44.924828, T: 3600, Avg. loss: 1.425079\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 112.43, NNZs: 600, Bias: 43.897108, T: 4200, Avg. loss: 1.204609\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 105.30, NNZs: 600, Bias: 40.691806, T: 4800, Avg. loss: 0.761550\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 100.60, NNZs: 600, Bias: 39.916314, T: 5400, Avg. loss: 0.717314\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 95.04, NNZs: 600, Bias: 40.411761, T: 6000, Avg. loss: 0.761669\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 90.49, NNZs: 600, Bias: 39.178061, T: 6600, Avg. loss: 0.786933\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 88.02, NNZs: 600, Bias: 37.155489, T: 7200, Avg. loss: 0.546967\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 84.48, NNZs: 600, Bias: 38.656786, T: 7800, Avg. loss: 0.646368\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 81.88, NNZs: 600, Bias: 38.425895, T: 8400, Avg. loss: 0.482841\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 79.38, NNZs: 600, Bias: 36.306967, T: 9000, Avg. loss: 0.493318\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 76.71, NNZs: 600, Bias: 36.173551, T: 9600, Avg. loss: 0.416934\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 73.93, NNZs: 600, Bias: 36.561704, T: 10200, Avg. loss: 0.394768\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 72.12, NNZs: 600, Bias: 36.381017, T: 10800, Avg. loss: 0.296066\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 70.30, NNZs: 600, Bias: 36.180319, T: 11400, Avg. loss: 0.298406\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 67.97, NNZs: 600, Bias: 37.536662, T: 12000, Avg. loss: 0.270409\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 66.85, NNZs: 600, Bias: 35.565670, T: 12600, Avg. loss: 0.247362\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 64.83, NNZs: 600, Bias: 35.113017, T: 13200, Avg. loss: 0.239522\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 63.79, NNZs: 600, Bias: 34.050892, T: 13800, Avg. loss: 0.245620\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 61.79, NNZs: 600, Bias: 35.048344, T: 14400, Avg. loss: 0.268296\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 60.84, NNZs: 600, Bias: 34.113606, T: 15000, Avg. loss: 0.241505\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 59.41, NNZs: 600, Bias: 34.329449, T: 15600, Avg. loss: 0.185577\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 58.32, NNZs: 600, Bias: 33.673231, T: 16200, Avg. loss: 0.217466\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 56.82, NNZs: 600, Bias: 33.807258, T: 16800, Avg. loss: 0.181244\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 56.18, NNZs: 600, Bias: 32.563955, T: 17400, Avg. loss: 0.210281\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 55.17, NNZs: 600, Bias: 32.495123, T: 18000, Avg. loss: 0.159224\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 54.05, NNZs: 600, Bias: 32.288147, T: 18600, Avg. loss: 0.159451\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 53.24, NNZs: 600, Bias: 31.811443, T: 19200, Avg. loss: 0.164554\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 52.13, NNZs: 600, Bias: 32.131199, T: 19800, Avg. loss: 0.164518\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 51.80, NNZs: 600, Bias: 31.048141, T: 20400, Avg. loss: 0.171537\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 50.44, NNZs: 600, Bias: 31.533440, T: 21000, Avg. loss: 0.147502\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 50.21, NNZs: 600, Bias: 30.377156, T: 21600, Avg. loss: 0.115823\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 48.89, NNZs: 600, Bias: 30.987331, T: 22200, Avg. loss: 0.146971\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 47.82, NNZs: 600, Bias: 31.263195, T: 22800, Avg. loss: 0.115246\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 47.55, NNZs: 600, Bias: 30.310968, T: 23400, Avg. loss: 0.147323\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 46.85, NNZs: 600, Bias: 30.123869, T: 24000, Avg. loss: 0.124942\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 46.03, NNZs: 600, Bias: 30.132163, T: 24600, Avg. loss: 0.156863\n",
      "Total training time: 0.06 seconds.\n",
      "Convergence after 41 epochs took 0.06 seconds\n",
      "['y_pred', 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import json\n",
    "\n",
    "sgd_output_file = \"./sgd_%s.json\" % dataset\n",
    "\n",
    "sgd_clf = SGDClassifier(loss='log', alpha=1e-4, random_state=0, verbose=1).fit(K_train, training_y)\n",
    "y_lr_holdout = sgd_clf.predict(K_validation)\n",
    "p_lr_holdout = sgd_clf.predict_proba(K_validation).T\n",
    "y_lr_test = sgd_clf.predict(K_test)\n",
    "p_lr_test = sgd_clf.predict_proba(K_test).T # Need to align the first axis with y_pred\n",
    "\n",
    "header_sgd = ['y_pred'] + sgd_clf.classes_.tolist()\n",
    "print(header_sgd)\n",
    "output_sgd_validation = np.vstack((y_lr_holdout, p_lr_holdout)).T\n",
    "output_sgd_test = np.vstack((y_lr_test, p_lr_test)).T\n",
    "output_sgd = {\n",
    "    'coef': sgd_clf.coef_.tolist(),\n",
    "    'intercept': sgd_clf.intercept_.tolist(),\n",
    "    'output_header': header_sgd,\n",
    "    'validation_output': output_sgd_validation.tolist(),\n",
    "    'test_output': output_sgd_test.tolist()\n",
    "}\n",
    "\n",
    "with open(sgd_output_file, 'w') as outfile:\n",
    "    json.dump(output_sgd, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.59542125\n",
      "Iteration 2, loss = 0.48002085\n",
      "Iteration 3, loss = 0.43638617\n",
      "Iteration 4, loss = 0.42482495\n",
      "Iteration 5, loss = 0.41758305\n",
      "Iteration 6, loss = 0.41323227\n",
      "Iteration 7, loss = 0.40841122\n",
      "Iteration 8, loss = 0.40301937\n",
      "Iteration 9, loss = 0.39635326\n",
      "Iteration 10, loss = 0.39044360\n",
      "Iteration 11, loss = 0.38482503\n",
      "Iteration 12, loss = 0.37969294\n",
      "Iteration 13, loss = 0.37663878\n",
      "Iteration 14, loss = 0.37191823\n",
      "Iteration 15, loss = 0.36789985\n",
      "Iteration 16, loss = 0.36445140\n",
      "Iteration 17, loss = 0.36051554\n",
      "Iteration 18, loss = 0.35615383\n",
      "Iteration 19, loss = 0.35256819\n",
      "Iteration 20, loss = 0.34826245\n",
      "Iteration 21, loss = 0.34414732\n",
      "Iteration 22, loss = 0.34034957\n",
      "Iteration 23, loss = 0.33707541\n",
      "Iteration 24, loss = 0.33177779\n",
      "Iteration 25, loss = 0.32757901\n",
      "Iteration 26, loss = 0.32394447\n",
      "Iteration 27, loss = 0.31977346\n",
      "Iteration 28, loss = 0.31477472\n",
      "Iteration 29, loss = 0.30986282\n",
      "Iteration 30, loss = 0.30567983\n",
      "Iteration 31, loss = 0.30084084\n",
      "Iteration 32, loss = 0.29608337\n",
      "Iteration 33, loss = 0.29187793\n",
      "Iteration 34, loss = 0.28644463\n",
      "Iteration 35, loss = 0.28345947\n",
      "Iteration 36, loss = 0.27973211\n",
      "Iteration 37, loss = 0.27556580\n",
      "Iteration 38, loss = 0.26876954\n",
      "Iteration 39, loss = 0.26361315\n",
      "Iteration 40, loss = 0.26260552\n",
      "Iteration 41, loss = 0.25909155\n",
      "Iteration 42, loss = 0.25151741\n",
      "Iteration 43, loss = 0.24702999\n",
      "Iteration 44, loss = 0.24525746\n",
      "Iteration 45, loss = 0.23845335\n",
      "Iteration 46, loss = 0.23444164\n",
      "Iteration 47, loss = 0.23073901\n",
      "Iteration 48, loss = 0.22753649\n",
      "Iteration 49, loss = 0.22317443\n",
      "Iteration 50, loss = 0.21973879\n",
      "Iteration 51, loss = 0.21701959\n",
      "Iteration 52, loss = 0.21258203\n",
      "Iteration 53, loss = 0.20893554\n",
      "Iteration 54, loss = 0.20544249\n",
      "Iteration 55, loss = 0.20489604\n",
      "Iteration 56, loss = 0.19877662\n",
      "Iteration 57, loss = 0.19635491\n",
      "Iteration 58, loss = 0.19330384\n",
      "Iteration 59, loss = 0.19042939\n",
      "Iteration 60, loss = 0.18839394\n",
      "Iteration 61, loss = 0.18270646\n",
      "Iteration 62, loss = 0.18232728\n",
      "Iteration 63, loss = 0.18189368\n",
      "Iteration 64, loss = 0.17578397\n",
      "Iteration 65, loss = 0.17626897\n",
      "Iteration 66, loss = 0.16984079\n",
      "Iteration 67, loss = 0.16927304\n",
      "Iteration 68, loss = 0.16702272\n",
      "Iteration 69, loss = 0.16291637\n",
      "Iteration 70, loss = 0.16119686\n",
      "Iteration 71, loss = 0.15782720\n",
      "Iteration 72, loss = 0.15485993\n",
      "Iteration 73, loss = 0.15408409\n",
      "Iteration 74, loss = 0.15023478\n",
      "Iteration 75, loss = 0.14805126\n",
      "Iteration 76, loss = 0.14757643\n",
      "Iteration 77, loss = 0.14524808\n",
      "Iteration 78, loss = 0.14359120\n",
      "Iteration 79, loss = 0.14191552\n",
      "Iteration 80, loss = 0.14006067\n",
      "Iteration 81, loss = 0.13719312\n",
      "Iteration 82, loss = 0.13537022\n",
      "Iteration 83, loss = 0.13444008\n",
      "Iteration 84, loss = 0.13234769\n",
      "Iteration 85, loss = 0.13053523\n",
      "Iteration 86, loss = 0.12897475\n",
      "Iteration 87, loss = 0.12710219\n",
      "Iteration 88, loss = 0.13002408\n",
      "Iteration 89, loss = 0.12573544\n",
      "Iteration 90, loss = 0.12431956\n",
      "Iteration 91, loss = 0.12370291\n",
      "Iteration 92, loss = 0.12109372\n",
      "Iteration 93, loss = 0.12024551\n",
      "Iteration 94, loss = 0.12003850\n",
      "Iteration 95, loss = 0.11651838\n",
      "Iteration 96, loss = 0.11536128\n",
      "Iteration 97, loss = 0.11680009\n",
      "Iteration 98, loss = 0.11563990\n",
      "Iteration 99, loss = 0.11360830\n",
      "Iteration 100, loss = 0.11223352\n",
      "Iteration 101, loss = 0.10920116\n",
      "Iteration 102, loss = 0.10806569\n",
      "Iteration 103, loss = 0.10842263\n",
      "Iteration 104, loss = 0.10615919\n",
      "Iteration 105, loss = 0.10449595\n",
      "Iteration 106, loss = 0.10366240\n",
      "Iteration 107, loss = 0.10325801\n",
      "Iteration 108, loss = 0.10653127\n",
      "Iteration 109, loss = 0.10173059\n",
      "Iteration 110, loss = 0.10053625\n",
      "Iteration 111, loss = 0.09998423\n",
      "Iteration 112, loss = 0.09718074\n",
      "Iteration 113, loss = 0.09736767\n",
      "Iteration 114, loss = 0.09764923\n",
      "Iteration 115, loss = 0.09945270\n",
      "Iteration 116, loss = 0.09648177\n",
      "Iteration 117, loss = 0.09458949\n",
      "Iteration 118, loss = 0.09601454\n",
      "Iteration 119, loss = 0.09372070\n",
      "Iteration 120, loss = 0.09216879\n",
      "Iteration 121, loss = 0.09206890\n",
      "Iteration 122, loss = 0.09196276\n",
      "Iteration 123, loss = 0.09088826\n",
      "Iteration 124, loss = 0.09012887\n",
      "Iteration 125, loss = 0.09023049\n",
      "Iteration 126, loss = 0.08818612\n",
      "Iteration 127, loss = 0.08950151\n",
      "Iteration 128, loss = 0.08640637\n",
      "Iteration 129, loss = 0.08750501\n",
      "Iteration 130, loss = 0.08611202\n",
      "Iteration 131, loss = 0.08502043\n",
      "Iteration 132, loss = 0.08453584\n",
      "Iteration 133, loss = 0.09085216\n",
      "Iteration 134, loss = 0.08180910\n",
      "Iteration 135, loss = 0.08690681\n",
      "Iteration 136, loss = 0.08468663\n",
      "Iteration 137, loss = 0.08154464\n",
      "Iteration 138, loss = 0.08037754\n",
      "Iteration 139, loss = 0.07989115\n",
      "Iteration 140, loss = 0.07937165\n",
      "Iteration 141, loss = 0.07724084\n",
      "Iteration 142, loss = 0.07933524\n",
      "Iteration 143, loss = 0.08028369\n",
      "Iteration 144, loss = 0.07691629\n",
      "Iteration 145, loss = 0.07855240\n",
      "Iteration 146, loss = 0.07554719\n",
      "Iteration 147, loss = 0.07556035\n",
      "Iteration 148, loss = 0.07523392\n",
      "Iteration 149, loss = 0.07305673\n",
      "Iteration 150, loss = 0.07440291\n",
      "Iteration 151, loss = 0.07217371\n",
      "Iteration 152, loss = 0.07344953\n",
      "Iteration 153, loss = 0.07089728\n",
      "Iteration 154, loss = 0.07119625\n",
      "Iteration 155, loss = 0.07279641\n",
      "Iteration 156, loss = 0.07036949\n",
      "Iteration 157, loss = 0.07139433\n",
      "Iteration 158, loss = 0.07077025\n",
      "Iteration 159, loss = 0.06948583\n",
      "Iteration 160, loss = 0.06940578\n",
      "Iteration 161, loss = 0.06948004\n",
      "Iteration 162, loss = 0.06884992\n",
      "Iteration 163, loss = 0.06817317\n",
      "Iteration 164, loss = 0.06620326\n",
      "Iteration 165, loss = 0.06733309\n",
      "Iteration 166, loss = 0.06559615\n",
      "Iteration 167, loss = 0.06595050\n",
      "Iteration 168, loss = 0.06461556\n",
      "Iteration 169, loss = 0.06852190\n",
      "Iteration 170, loss = 0.06452040\n",
      "Iteration 171, loss = 0.06459728\n",
      "Iteration 172, loss = 0.06281760\n",
      "Iteration 173, loss = 0.06314894\n",
      "Iteration 174, loss = 0.06244343\n",
      "Iteration 175, loss = 0.06388988\n",
      "Iteration 176, loss = 0.06105189\n",
      "Iteration 177, loss = 0.06273151\n",
      "Iteration 178, loss = 0.06179454\n",
      "Iteration 179, loss = 0.06156545\n",
      "Iteration 180, loss = 0.06285883\n",
      "Iteration 181, loss = 0.06058577\n",
      "Iteration 182, loss = 0.06255886\n",
      "Iteration 183, loss = 0.05820593\n",
      "Iteration 184, loss = 0.06117067\n",
      "Iteration 185, loss = 0.05945071\n",
      "Iteration 186, loss = 0.05822879\n",
      "Iteration 187, loss = 0.05904226\n",
      "Iteration 188, loss = 0.05731397\n",
      "Iteration 189, loss = 0.05745252\n",
      "Iteration 190, loss = 0.05734875\n",
      "Iteration 191, loss = 0.05656672\n",
      "Iteration 192, loss = 0.05657517\n",
      "Iteration 193, loss = 0.05621086\n",
      "Iteration 194, loss = 0.05601551\n",
      "Iteration 195, loss = 0.05638929\n",
      "Iteration 196, loss = 0.05446860\n",
      "Iteration 197, loss = 0.05487720\n",
      "Iteration 198, loss = 0.05394498\n",
      "Iteration 199, loss = 0.05434562\n",
      "Iteration 200, loss = 0.05395644\n",
      "Iteration 201, loss = 0.05511806\n",
      "Iteration 202, loss = 0.05352592\n",
      "Iteration 203, loss = 0.05380698\n",
      "Iteration 204, loss = 0.05522537\n",
      "Iteration 205, loss = 0.05288256\n",
      "Iteration 206, loss = 0.05509738\n",
      "Iteration 207, loss = 0.05248345\n",
      "Iteration 208, loss = 0.05247452\n",
      "Iteration 209, loss = 0.05297609\n",
      "Iteration 210, loss = 0.05191124\n",
      "Iteration 211, loss = 0.05127173\n",
      "Iteration 212, loss = 0.05304003\n",
      "Iteration 213, loss = 0.05330390\n",
      "Iteration 214, loss = 0.05102241\n",
      "Iteration 215, loss = 0.05256401\n",
      "Iteration 216, loss = 0.05211937\n",
      "Iteration 217, loss = 0.05133235\n",
      "Iteration 218, loss = 0.05036368\n",
      "Iteration 219, loss = 0.05098776\n",
      "Iteration 220, loss = 0.05106199\n",
      "Iteration 221, loss = 0.04912376\n",
      "Iteration 222, loss = 0.04865497\n",
      "Iteration 223, loss = 0.04798256\n",
      "Iteration 224, loss = 0.04752153\n",
      "Iteration 225, loss = 0.04773073\n",
      "Iteration 226, loss = 0.04745953\n",
      "Iteration 227, loss = 0.04832603\n",
      "Iteration 228, loss = 0.04909026\n",
      "Iteration 229, loss = 0.04790307\n",
      "Iteration 230, loss = 0.04806674\n",
      "Iteration 231, loss = 0.05002346\n",
      "Iteration 232, loss = 0.05432941\n",
      "Iteration 233, loss = 0.05044462\n",
      "Iteration 234, loss = 0.04759392\n",
      "Iteration 235, loss = 0.04764562\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "['y_pred', 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_output_file = \"./mlp_%s.json\" % dataset\n",
    "\n",
    "# TODO: Do we need to transform the output from {-1, 1} to {0, 1}?\n",
    "mlp_clf = MLPClassifier(solver='adam', alpha=1e-4, activation='relu',hidden_layer_sizes=(256, 256),\n",
    "                        random_state=1, max_iter=1000, verbose=True).fit(training_X, training_y)\n",
    "\n",
    "y_mlp_validation = mlp_clf.predict(validation_X)\n",
    "p_mlp_validation = mlp_clf.predict_proba(validation_X).T\n",
    "\n",
    "y_mlp_test = mlp_clf.predict(test_X)\n",
    "p_mlp_test = mlp_clf.predict_proba(test_X).T\n",
    "\n",
    "header_mlp = ['y_pred'] + mlp_clf.classes_.tolist()\n",
    "print(header_mlp)\n",
    "output_mlp_validation = np.vstack((y_mlp_validation, p_mlp_validation)).T\n",
    "output_mlp_test = np.vstack((y_mlp_test, p_mlp_test)).T\n",
    "\n",
    "coefs = []\n",
    "intercepts = []\n",
    "for coef in mlp_clf.coefs_:\n",
    "    coefs.append(coef.tolist())\n",
    "\n",
    "for intercept in mlp_clf.intercepts_:\n",
    "    intercepts.append(intercept.tolist())\n",
    "\n",
    "output_mlp = {\n",
    "    'coef': coefs,\n",
    "    'intercept': intercepts,\n",
    "    'output_header': header_mlp,\n",
    "    'validation_output': output_mlp_validation.tolist(),\n",
    "    'test_output': output_mlp_test.tolist()\n",
    "}\n",
    "\n",
    "with open(mlp_output_file, 'w') as outfile:\n",
    "    json.dump(output_mlp, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 256)\n",
      "(256, 256)\n",
      "(256, 1)\n",
      "(256,)\n",
      "(256,)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "for coef in mlp_clf.coefs_:\n",
    "    print(coef.shape,)\n",
    "    \n",
    "for b in mlp_clf.intercepts_:\n",
    "    print(b.shape,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ConfidenceScore] *",
   "language": "python",
   "name": "conda-env-ConfidenceScore-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
