{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 2)\n",
      "(1600,)\n",
      "(400, 2)\n",
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset = \"CSpace1\"\n",
    "\n",
    "training_file = \"./data/%s_training.csv\" % dataset\n",
    "test_file = \"./data/%s_test.csv\" % dataset\n",
    "\n",
    "\n",
    "training_data = np.loadtxt(training_file, skiprows=3, delimiter=',')\n",
    "test_data = np.loadtxt(test_file, skiprows=3, delimiter=',')\n",
    "\n",
    "training_X = training_data[:, 0:2]\n",
    "training_y = training_data[:, 2]\n",
    "# training_y[training_y == -1] = 0 # convert y to 0;\n",
    "\n",
    "test_X = test_data[:, 0:2]\n",
    "test_y = test_data[:, 2]\n",
    "# test_y[test_y == -1] = 0 # convert y to 0;\n",
    "\n",
    "print(training_X.shape)\n",
    "print(training_y.shape)\n",
    "\n",
    "print(test_X.shape,)\n",
    "print(test_y.shape,)\n",
    "\n",
    "# print(\"Number of positive vs negative samples: %s vs %s\" % (np.count_nonzero(training_y == 1), np.count_nonzero(training_y == -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y_pred', -1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import json\n",
    "\n",
    "lr_output_file = \"./logRegression_%s.json\" % dataset\n",
    "\n",
    "lr_clf = LogisticRegression(random_state=0, solver='lbfgs',verbose=1).fit(training_X, training_y)\n",
    "y_lr = lr_clf.predict(test_X)\n",
    "p_lr = lr_clf.predict_proba(test_X).T # Need to align the first axis with y_pred\n",
    "\n",
    "header_lr = ['y_pred'] + lr_clf.classes_.tolist()\n",
    "print(header_lr)\n",
    "output_lr_array = np.vstack((y_lr, p_lr)).T\n",
    "output_lr = {\n",
    "    'coef': lr_clf.coef_.tolist(),\n",
    "    'intercept': lr_clf.intercept_.tolist(),\n",
    "    'test_output_header': header_lr,\n",
    "    'test_output': output_lr_array.tolist()\n",
    "}\n",
    "\n",
    "with open(lr_output_file, 'w') as outfile:\n",
    "    json.dump(output_lr, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 3.33, NNZs: 2, Bias: 1.849983, T: 1600, Avg. loss: 3.040701\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.91, NNZs: 2, Bias: -0.896838, T: 3200, Avg. loss: 1.411780\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.46, NNZs: 2, Bias: -1.705348, T: 4800, Avg. loss: 1.001597\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.50, NNZs: 2, Bias: -2.449151, T: 6400, Avg. loss: 0.817565\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.18, NNZs: 2, Bias: -1.863430, T: 8000, Avg. loss: 0.703656\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.85, NNZs: 2, Bias: -0.978572, T: 9600, Avg. loss: 0.656215\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.07, NNZs: 2, Bias: -0.901788, T: 11200, Avg. loss: 0.620853\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.22, NNZs: 2, Bias: -2.761558, T: 12800, Avg. loss: 0.606187\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.13, NNZs: 2, Bias: -1.815312, T: 14400, Avg. loss: 0.580233\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.63, NNZs: 2, Bias: -0.512208, T: 16000, Avg. loss: 0.575970\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.16, NNZs: 2, Bias: -0.953081, T: 17600, Avg. loss: 0.554816\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.44, NNZs: 2, Bias: -2.192414, T: 19200, Avg. loss: 0.559563\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.28, NNZs: 2, Bias: -1.422645, T: 20800, Avg. loss: 0.558708\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.61, NNZs: 2, Bias: -1.000688, T: 22400, Avg. loss: 0.548050\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.83, NNZs: 2, Bias: -1.011261, T: 24000, Avg. loss: 0.542512\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.53, NNZs: 2, Bias: -1.126512, T: 25600, Avg. loss: 0.542373\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.32, NNZs: 2, Bias: -1.454623, T: 27200, Avg. loss: 0.539697\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.18, NNZs: 2, Bias: -1.327943, T: 28800, Avg. loss: 0.538618\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.73, NNZs: 2, Bias: -1.260785, T: 30400, Avg. loss: 0.530267\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.42, NNZs: 2, Bias: -1.088179, T: 32000, Avg. loss: 0.530579\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.10, NNZs: 2, Bias: -1.478203, T: 33600, Avg. loss: 0.531537\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.17, NNZs: 2, Bias: -2.183108, T: 35200, Avg. loss: 0.530834\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 0.40, NNZs: 2, Bias: -1.320798, T: 36800, Avg. loss: 0.525512\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 0.37, NNZs: 2, Bias: -1.219597, T: 38400, Avg. loss: 0.527065\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.50, NNZs: 2, Bias: -0.869155, T: 40000, Avg. loss: 0.522584\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 0.34, NNZs: 2, Bias: -1.087839, T: 41600, Avg. loss: 0.524911\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 0.68, NNZs: 2, Bias: -1.056812, T: 43200, Avg. loss: 0.518855\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 0.32, NNZs: 2, Bias: -0.971433, T: 44800, Avg. loss: 0.520725\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 0.31, NNZs: 2, Bias: -1.998283, T: 46400, Avg. loss: 0.517464\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 0.18, NNZs: 2, Bias: -1.713894, T: 48000, Avg. loss: 0.522956\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 0.57, NNZs: 2, Bias: -1.365608, T: 49600, Avg. loss: 0.523709\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 0.32, NNZs: 2, Bias: -1.644887, T: 51200, Avg. loss: 0.517631\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 0.35, NNZs: 2, Bias: -1.116277, T: 52800, Avg. loss: 0.518443\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 0.33, NNZs: 2, Bias: -0.835572, T: 54400, Avg. loss: 0.519351\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 34 epochs took 0.02 seconds\n",
      "['y_pred', -1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import json\n",
    "\n",
    "sgd_output_file = \"./sgd_%s.json\" % dataset\n",
    "\n",
    "sgd_clf = SGDClassifier(loss='log', alpha=1e-4, random_state=0, verbose=1).fit(training_X, training_y)\n",
    "y_lr = sgd_clf.predict(test_X)\n",
    "p_lr = sgd_clf.predict_proba(test_X).T # Need to align the first axis with y_pred\n",
    "\n",
    "header_sgd = ['y_pred'] + sgd_clf.classes_.tolist()\n",
    "print(header_lr)\n",
    "output_sgd_array = np.vstack((y_lr, p_lr)).T\n",
    "output_sgd = {\n",
    "    'coef': sgd_clf.coef_.tolist(),\n",
    "    'intercept': sgd_clf.intercept_.tolist(),\n",
    "    'test_output_header': header_sgd,\n",
    "    'test_output': output_sgd_array.tolist()\n",
    "}\n",
    "\n",
    "with open(sgd_output_file, 'w') as outfile:\n",
    "    json.dump(output_sgd, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.54661467\n",
      "Iteration 2, loss = 0.39732597\n",
      "Iteration 3, loss = 0.36984004\n",
      "Iteration 4, loss = 0.32599589\n",
      "Iteration 5, loss = 0.28761793\n",
      "Iteration 6, loss = 0.24836735\n",
      "Iteration 7, loss = 0.21164702\n",
      "Iteration 8, loss = 0.17738655\n",
      "Iteration 9, loss = 0.14763241\n",
      "Iteration 10, loss = 0.12418803\n",
      "Iteration 11, loss = 0.10416211\n",
      "Iteration 12, loss = 0.08914514\n",
      "Iteration 13, loss = 0.07674189\n",
      "Iteration 14, loss = 0.06754846\n",
      "Iteration 15, loss = 0.06019104\n",
      "Iteration 16, loss = 0.05450723\n",
      "Iteration 17, loss = 0.04995987\n",
      "Iteration 18, loss = 0.04628980\n",
      "Iteration 19, loss = 0.04283084\n",
      "Iteration 20, loss = 0.04036279\n",
      "Iteration 21, loss = 0.03796774\n",
      "Iteration 22, loss = 0.03612794\n",
      "Iteration 23, loss = 0.03455014\n",
      "Iteration 24, loss = 0.03287674\n",
      "Iteration 25, loss = 0.03078394\n",
      "Iteration 26, loss = 0.02942503\n",
      "Iteration 27, loss = 0.02843009\n",
      "Iteration 28, loss = 0.02823579\n",
      "Iteration 29, loss = 0.02639043\n",
      "Iteration 30, loss = 0.02565600\n",
      "Iteration 31, loss = 0.02528919\n",
      "Iteration 32, loss = 0.02397271\n",
      "Iteration 33, loss = 0.02311874\n",
      "Iteration 34, loss = 0.02248514\n",
      "Iteration 35, loss = 0.02228713\n",
      "Iteration 36, loss = 0.02180582\n",
      "Iteration 37, loss = 0.02144230\n",
      "Iteration 38, loss = 0.02078200\n",
      "Iteration 39, loss = 0.01992778\n",
      "Iteration 40, loss = 0.01911564\n",
      "Iteration 41, loss = 0.01869396\n",
      "Iteration 42, loss = 0.01824630\n",
      "Iteration 43, loss = 0.01876004\n",
      "Iteration 44, loss = 0.01749493\n",
      "Iteration 45, loss = 0.01774658\n",
      "Iteration 46, loss = 0.01707683\n",
      "Iteration 47, loss = 0.01659953\n",
      "Iteration 48, loss = 0.01701423\n",
      "Iteration 49, loss = 0.01558138\n",
      "Iteration 50, loss = 0.01541330\n",
      "Iteration 51, loss = 0.01508549\n",
      "Iteration 52, loss = 0.01460527\n",
      "Iteration 53, loss = 0.01435231\n",
      "Iteration 54, loss = 0.01400275\n",
      "Iteration 55, loss = 0.01406971\n",
      "Iteration 56, loss = 0.01346660\n",
      "Iteration 57, loss = 0.01385377\n",
      "Iteration 58, loss = 0.01313907\n",
      "Iteration 59, loss = 0.01318655\n",
      "Iteration 60, loss = 0.01265245\n",
      "Iteration 61, loss = 0.01218305\n",
      "Iteration 62, loss = 0.01191525\n",
      "Iteration 63, loss = 0.01202137\n",
      "Iteration 64, loss = 0.01176017\n",
      "Iteration 65, loss = 0.01215980\n",
      "Iteration 66, loss = 0.01178458\n",
      "Iteration 67, loss = 0.01198765\n",
      "Iteration 68, loss = 0.01410842\n",
      "Iteration 69, loss = 0.01337332\n",
      "Iteration 70, loss = 0.01200178\n",
      "Iteration 71, loss = 0.01142352\n",
      "Iteration 72, loss = 0.01078683\n",
      "Iteration 73, loss = 0.01009109\n",
      "Iteration 74, loss = 0.01071942\n",
      "Iteration 75, loss = 0.00933825\n",
      "Iteration 76, loss = 0.00964697\n",
      "Iteration 77, loss = 0.00992401\n",
      "Iteration 78, loss = 0.00967137\n",
      "Iteration 79, loss = 0.00994606\n",
      "Iteration 80, loss = 0.00994251\n",
      "Iteration 81, loss = 0.00986812\n",
      "Iteration 82, loss = 0.01128702\n",
      "Iteration 83, loss = 0.01007776\n",
      "Iteration 84, loss = 0.00946580\n",
      "Iteration 85, loss = 0.00850496\n",
      "Iteration 86, loss = 0.00839515\n",
      "Iteration 87, loss = 0.00851597\n",
      "Iteration 88, loss = 0.00778549\n",
      "Iteration 89, loss = 0.00869951\n",
      "Iteration 90, loss = 0.00765810\n",
      "Iteration 91, loss = 0.00779797\n",
      "Iteration 92, loss = 0.00745797\n",
      "Iteration 93, loss = 0.00765720\n",
      "Iteration 94, loss = 0.00766082\n",
      "Iteration 95, loss = 0.00742824\n",
      "Iteration 96, loss = 0.00713274\n",
      "Iteration 97, loss = 0.00733361\n",
      "Iteration 98, loss = 0.00673514\n",
      "Iteration 99, loss = 0.00707362\n",
      "Iteration 100, loss = 0.00715934\n",
      "Iteration 101, loss = 0.00721491\n",
      "Iteration 102, loss = 0.00784200\n",
      "Iteration 103, loss = 0.00669334\n",
      "Iteration 104, loss = 0.00687039\n",
      "Iteration 105, loss = 0.00661377\n",
      "Iteration 106, loss = 0.00692388\n",
      "Iteration 107, loss = 0.00642139\n",
      "Iteration 108, loss = 0.00628319\n",
      "Iteration 109, loss = 0.00643438\n",
      "Iteration 110, loss = 0.00634754\n",
      "Iteration 111, loss = 0.00725832\n",
      "Iteration 112, loss = 0.00632754\n",
      "Iteration 113, loss = 0.00649722\n",
      "Iteration 114, loss = 0.00655575\n",
      "Iteration 115, loss = 0.00631856\n",
      "Iteration 116, loss = 0.00587317\n",
      "Iteration 117, loss = 0.00551777\n",
      "Iteration 118, loss = 0.00565054\n",
      "Iteration 119, loss = 0.00587263\n",
      "Iteration 120, loss = 0.00513817\n",
      "Iteration 121, loss = 0.00546041\n",
      "Iteration 122, loss = 0.00495285\n",
      "Iteration 123, loss = 0.00571826\n",
      "Iteration 124, loss = 0.00515274\n",
      "Iteration 125, loss = 0.00513727\n",
      "Iteration 126, loss = 0.00518598\n",
      "Iteration 127, loss = 0.00496797\n",
      "Iteration 128, loss = 0.00483938\n",
      "Iteration 129, loss = 0.00488210\n",
      "Iteration 130, loss = 0.00454397\n",
      "Iteration 131, loss = 0.00493601\n",
      "Iteration 132, loss = 0.00501818\n",
      "Iteration 133, loss = 0.00457426\n",
      "Iteration 134, loss = 0.00466442\n",
      "Iteration 135, loss = 0.00440793\n",
      "Iteration 136, loss = 0.00484741\n",
      "Iteration 137, loss = 0.00496658\n",
      "Iteration 138, loss = 0.00447483\n",
      "Iteration 139, loss = 0.00416510\n",
      "Iteration 140, loss = 0.00456778\n",
      "Iteration 141, loss = 0.00447876\n",
      "Iteration 142, loss = 0.00440743\n",
      "Iteration 143, loss = 0.00405691\n",
      "Iteration 144, loss = 0.00407659\n",
      "Iteration 145, loss = 0.00533147\n",
      "Iteration 146, loss = 0.00415547\n",
      "Iteration 147, loss = 0.00406268\n",
      "Iteration 148, loss = 0.00388620\n",
      "Iteration 149, loss = 0.00386775\n",
      "Iteration 150, loss = 0.00394587\n",
      "Iteration 151, loss = 0.00409342\n",
      "Iteration 152, loss = 0.00375800\n",
      "Iteration 153, loss = 0.00367434\n",
      "Iteration 154, loss = 0.00382349\n",
      "Iteration 155, loss = 0.00358273\n",
      "Iteration 156, loss = 0.00353800\n",
      "Iteration 157, loss = 0.00339317\n",
      "Iteration 158, loss = 0.00379827\n",
      "Iteration 159, loss = 0.00441609\n",
      "Iteration 160, loss = 0.00394879\n",
      "Iteration 161, loss = 0.00331303\n",
      "Iteration 162, loss = 0.00389620\n",
      "Iteration 163, loss = 0.00334940\n",
      "Iteration 164, loss = 0.00340446\n",
      "Iteration 165, loss = 0.00359644\n",
      "Iteration 166, loss = 0.00318475\n",
      "Iteration 167, loss = 0.00328459\n",
      "Iteration 168, loss = 0.00367961\n",
      "Iteration 169, loss = 0.00327753\n",
      "Iteration 170, loss = 0.00297806\n",
      "Iteration 171, loss = 0.00385020\n",
      "Iteration 172, loss = 0.00410042\n",
      "Iteration 173, loss = 0.00421565\n",
      "Iteration 174, loss = 0.00349523\n",
      "Iteration 175, loss = 0.00401485\n",
      "Iteration 176, loss = 0.00305715\n",
      "Iteration 177, loss = 0.00308248\n",
      "Iteration 178, loss = 0.00302271\n",
      "Iteration 179, loss = 0.00303536\n",
      "Iteration 180, loss = 0.00283971\n",
      "Iteration 181, loss = 0.00331942\n",
      "Iteration 182, loss = 0.00264763\n",
      "Iteration 183, loss = 0.00337572\n",
      "Iteration 184, loss = 0.00303817\n",
      "Iteration 185, loss = 0.00293383\n",
      "Iteration 186, loss = 0.00279728\n",
      "Iteration 187, loss = 0.00281354\n",
      "Iteration 188, loss = 0.00345505\n",
      "Iteration 189, loss = 0.00335550\n",
      "Iteration 190, loss = 0.00361996\n",
      "Iteration 191, loss = 0.00386292\n",
      "Iteration 192, loss = 0.00308416\n",
      "Iteration 193, loss = 0.00299648\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "['y_pred', -1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_output_file = \"./mlp_%s.json\" % dataset\n",
    "\n",
    "# TODO: Do we need to transform the output from {-1, 1} to {0, 1}?\n",
    "mlp_clf = MLPClassifier(solver='adam', alpha=1e-4, activation='relu',hidden_layer_sizes=(256, 256),\n",
    "                        random_state=1, max_iter=1000, verbose=True).fit(training_X, training_y)\n",
    "y_mlp = mlp_clf.predict(test_X)\n",
    "p_mlp = mlp_clf.predict_proba(test_X).T\n",
    "\n",
    "header_mlp = ['y_pred'] + mlp_clf.classes_.tolist()\n",
    "print(header_mlp)\n",
    "output_mlp_array = np.vstack((y_mlp, p_mlp)).T\n",
    "\n",
    "coefs = []\n",
    "intercepts = []\n",
    "for coef in mlp_clf.coefs_:\n",
    "    coefs.append(coef.tolist())\n",
    "\n",
    "for intercept in mlp_clf.intercepts_:\n",
    "    intercepts.append(intercept.tolist())\n",
    "\n",
    "output_mlp = {\n",
    "    'coef': coefs,\n",
    "    'intercept': intercepts,\n",
    "    'test_output_header': header_mlp,\n",
    "    'test_output': output_mlp_array.tolist()\n",
    "}\n",
    "\n",
    "with open(mlp_output_file, 'w') as outfile:\n",
    "    json.dump(output_mlp, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 256)\n",
      "(256, 256)\n",
      "(256, 1)\n",
      "(256,)\n",
      "(256,)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "for coef in mlp_clf.coefs_:\n",
    "    print(coef.shape,)\n",
    "    \n",
    "for b in mlp_clf.intercepts_:\n",
    "    print(b.shape,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ConfidenceScore] *",
   "language": "python",
   "name": "conda-env-ConfidenceScore-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
